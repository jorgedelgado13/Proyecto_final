---
title: "Laboratorio"
code-fold: false
---

# Importar librerías

```{python}
import pandas as pd
import altair as alt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder,OrdinalEncoder,StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk import word_tokenize # tokenizacion
from nltk import pos_tag #lematizacion
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()

nltk.download('stopwords') # necessary for removal of stop words
nltk.download('wordnet')
```


# Cargar data

```{python}
tweets = pd.read_csv("data/tweets_totales_con_sentimiento_ml.csv")

# Muestra las primeras filas
print(tweets.head)
print(tweets.info)
print(tweets.shape)
print(tweets.dtypes)
```
Entender la estructura del dataset nos da una mejor idea de con que estamos lidiando

### Porcentaje de Missing Values por Columna

```{python}
nan_percent = tweets.isna().mean() * 100
nan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)
nan_percent_sorted
```


# Asignar nueva columna 
```{python}
tweets['es_respuesta'] = LabelEncoder().fit_transform(tweets['isReply'])
tweets['autor_verificado'] = LabelEncoder().fit_transform(tweets['authorVerified'])
tweets['tiene_foto'] = LabelEncoder().fit_transform(tweets['has_profile_picture'])
tweets.head()
```


# CountVectorizer
```{python}
stop_words = stopwords.words('english')
vectorizer = CountVectorizer(stop_words=stop_words)
#vectorizer = TfidfVectorizer(stop_words=stop_words)
```


## fit: Aprende el vocabulari
```{python}
X = vectorizer.fit_transform(tweets)
```

```{python}
vectorizer.get_feature_names_out()
```

## transform: Crea la matriz de conteos (sparse matrix)
Cada fila representa un documento, cada columna una palabra del vocabulario
```{python}
X.toarray()
```

## Visualizar la matriz como DataFrame

```{python}
df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print(df)
```

    
## Función de limpieza

```{python}



def get_wordnet_pos(treebank_tag):
    # print("treebank_tag",treebank_tag)
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN 


def preprocessing_document(doc):
    #1 transformar en minusculas
    doc = doc.lower()
    
    #2 tokenizar
    tokens = word_tokenize(doc)

    #3 obtener lematizacion con etiqueta POS
    tagged_tokens = pos_tag(tokens)

    #ELIMINAR LAS STOP-WORDS EN CASO QUE NO USE EL PARAMETRO STOPWORD EN LA VECTORIZACION

    #4 filtrar numeros
    filtered_tokens = [(word,pos) for (word,pos) in tagged_tokens if word.isalpha()]

    #5 Lematizacion usando el pos
    lemmatized_words = [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word, pos in filtered_tokens]

    return " ".join(lemmatized_words)

```

# Preprocesamiento manual

```{python}
for doc in tweets:
    r = preprocessing_document(doc)
    print(f"doc: {doc}, preprocesamiento: {r}")
```

## Fit_transform
Aprende de los tweets (vocabulario) y del ser caso elimina las stop-words


```{python}
tweets_cleaned = [preprocessing_document(doc) for doc in tweets]
X = vectorizer.fit_transform(tweets)
```

## Obtener las palabras finales

```{python}
vectorizer.get_feature_names_out()
```

# Matriz de ocurrencias (conteos)
Esta matriz muestra cada fila un documento, y cada columna una palabra

```{python}
X.toarray()
```

```{python}


df_1 = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())
df_1
```
