---
title: "Laboratorio"
code-fold: false
---

# Importar librer√≠as

```{python}
import pandas as pd
import altair as alt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder,OrdinalEncoder,StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk import word_tokenize # tokenizacion
from nltk import pos_tag #lematizacion
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()

nltk.download('stopwords') # necessary for removal of stop words
nltk.download('wordnet')
```


# Cargar data

```{python}
tweets = pd.read_csv("data/tweets_totales_con_sentimiento_ml.csv")

# Muestra las primeras filas
print(tweets.head)
print(tweets.info)
print(tweets.shape)
print(tweets.dtypes)
```
Entender la estructura del dataset nos da una mejor idea de con que estamos lidiando

### Porcentaje de Missing Values por Columna

```{python}
nan_percent = tweets.isna().mean() * 100
nan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)
nan_percent_sorted
```


# Asignar nueva columna 
```{python}
tweets['es_respuesta'] = LabelEncoder().fit_transform(tweets['isReply'])
tweets['autor_verificado'] = LabelEncoder().fit_transform(tweets['authorVerified'])
tweets['tiene_foto'] = LabelEncoder().fit_transform(tweets['has_profile_picture'])
tweets.head()
```


# CountVectorizer
```{python}
stop_words = stopwords.words('english')
vectorizer = CountVectorizer(stop_words=stop_words)
```


## fit: Aprende el vocabulari
```{python}
X = vectorizer.fit_transform(tweets[content])
```

```{python}
vectorizer.get_feature_names_out()
```

## transform: Crea la matriz de conteos (sparse matrix)
Cada fila representa un documento, cada columna una palabra del vocabulario
```{python}
X.toarray()
```

## Visualizar la matriz como DataFrame

```{python}
df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print(df)
```