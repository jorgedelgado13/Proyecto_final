---
title: "Laboratorio"
code-fold: false
---

# Importar librerías

```{python}
# ====== Importar librerías ======
import re
import unicodedata
import numpy as np
import pandas as pd
import altair as alt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, ConfusionMatrixDisplay
)

# NLTK
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)
stop_es = stopwords.words("spanish")
```


# Cargar data

```{python}
tweets = pd.read_csv("data/tweets_totales_con_sentimiento_ml.csv", encoding="utf-8")

# Vista rápida
display(tweets.head(3))
tweets.info()
print("shape:", tweets.shape)
print(tweets.dtypes)
```
Entender la estructura del dataset nos da una mejor idea de con que estamos lidiando

### Porcentaje de Missing Values por Columna

```{python}
# ====== Porcentaje de Missing Values por Columna ======
nan_percent = tweets.isna().mean().sort_values(ascending=False).mul(100).round(2)
nan_percent.head(20)


```



## Función de limpieza

```{python}

def normalize_text(text: str) -> str:
    text = str(text)
    text = text.lower().strip()
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)  # URLs
    text = re.sub(r"@\w+", " ", text)                   # menciones
    text = re.sub(r"#\w+", " ", text)                   # hashtags (como texto)
    text = re.sub(r"[^a-zñ\s]", " ", text)              # solo letras y espacios
    text = re.sub(r"\s+", " ", text).strip()            # espacios extra
    return text


tweets["clean_text"] = tweets["content"].fillna("").astype(str).apply(normalize_text)

# Creación de algunas features simples de texto
tweets["num_urls"]      = tweets["content"].fillna("").str.count(r"https?://|www\.")
tweets["num_mentions"]  = tweets["content"].fillna("").str.count(r"@\w+")
tweets["num_hashtags"]  = tweets["content"].fillna("").str.count(r"#\w+")
tweets["text_len"]      = tweets["clean_text"].str.len()

tweets[["content","clean_text","num_urls","num_mentions","num_hashtags","text_len"]].head(5)

```

## Palabras que demuestran apoyo o disgusto

```{python}
lex_neg = {
    "asco","basura","corrupto","estupido","idiota","ladron","mentiroso","odiar","odio",
    "imbecil","tonto","vomito","repugnante","asqueroso","miserable"
}
lex_pos = {
    "apoyo","gracias","excelente","felicitaciones","bien","bravo","genial","mejor",
    "aplaudo","orgullo","feliz","contento","admiro","fuerza","vamos"
}

def count_lexicon_hits(text: str, lexicon: set) -> int:
    if not text:
        return 0
    tokens = word_tokenize(text, language="spanish")
    return sum(1 for t in tokens if t in lexicon)

tweets["neg_count"] = tweets["clean_text"].apply(lambda t: count_lexicon_hits(t, lex_neg))
tweets["pos_count"] = tweets["clean_text"].apply(lambda t: count_lexicon_hits(t, lex_pos))

tweets[["clean_text","neg_count","pos_count"]].head(5)
```

## Creación de etiquetas con LabelEncoder
```{python}
# Columnas esperadas, se crean como 0 para evitar errores
cols_flags = ["isReply", "authorVerified", "has_profile_picture"]
for c in cols_flags:
    if c not in tweets.columns:
        tweets[c] = 0

# Crear nuevas columnas *_encoded usando LabelEncoder 
for c in cols_flags:
    le = LabelEncoder()
    # Convertimos a string por robustez ante valores no booleanos
    tweets[f"{c}_encoded"] = le.fit_transform(tweets[c].astype(str))

tweets[[f"{c}_encoded" for c in cols_flags]].head(5)

```

## Construcción de variable target basada en caractisticas de usuarios
```{python}
# Asegurar columnas numéricas, si no existen, se crean con 0
num_needed = ["account_age_days", "time_response"]
for c in num_needed:
    if c not in tweets.columns:
        tweets[c] = 0

# Cast numérico seguro
for c in ["account_age_days","time_response","num_urls","num_mentions","num_hashtags",
          "text_len","neg_count","pos_count",
          "isReply_encoded","authorVerified_encoded","has_profile_picture_encoded"]:
    tweets[c] = pd.to_numeric(tweets[c], errors="coerce").fillna(0)

# Reglas -> puntos
score = np.zeros(len(tweets), dtype=int)

score += (tweets["account_age_days"] < 30).astype(int)                   # cuenta reciente
score += (tweets["time_response"] < 30).astype(int)                      # respuesta muy rápida (ajusta a tu caso)
score += (tweets["authorVerified_encoded"] == 0).astype(int)             # no verificada
score += (tweets["has_profile_picture_encoded"] == 0).astype(int)        # sin foto
score += ((tweets["num_urls"] + tweets["num_mentions"] + tweets["num_hashtags"]) >= 3).astype(int)  # mucho "ruido"
score += ((tweets["neg_count"] > tweets["pos_count"]) & (tweets["neg_count"] >= 2)).astype(int)     # negatividad marcada

# Umbral: 3 señales o más => bot
tweets["bots"] = (score >= 3).astype(int)

print("Distribución de 'bots':")
print(tweets["bots"].value_counts(dropna=False))

```

## Vectorización de texto 
```{python}
# Vectorizer sobre 'clean_text' (español)
# vectorizer = CountVectorizer(stop_words=stop_es) 
vectorizer = TfidfVectorizer(stop_words=stop_es)
X_text = vectorizer.fit_transform(tweets["clean_text"])
vocab   = vectorizer.get_feature_names_out()
print("Matriz TF-IDF:", X_text.shape)
print("Ejemplo de vocab:", vocab[:20])

```

## One hot encoder para variables categoricas (método de autodetección)
```{python}
# Detectar automáticamente columnas object para One-Hot (excluyendo 'content'/'clean_text')
obj_cols = [c for c in tweets.select_dtypes(include=["object"]).columns
            if c not in {"content","clean_text"}]

obj_cols[:10]

```

## Ensamble: númericas (Label Ecoder) + categóticas (One-hot-encoder) + text (columntransformer)
```{python}
# Numéricas explícitas (puedes añadir/quitar)
num_cols = [
    "account_age_days","time_response","num_urls","num_mentions","num_hashtags","text_len",
    "neg_count","pos_count",
    # Flags codificados por LabelEncoder (ya son numéricos)
    "isReply_encoded","authorVerified_encoded","has_profile_picture_encoded"
]

# Algunas podrían no existir (si el dataset original varía)
num_cols = [c for c in num_cols if c in tweets.columns]

# Preprocesador
preprocessor = ColumnTransformer(
    transformers=[
        ("num",  Pipeline([("scaler", StandardScaler())]), num_cols),
        ("cat",  OneHotEncoder(handle_unknown="ignore", sparse_output=True), obj_cols),
        ("text", vectorizer, "clean_text"),
    ],
    remainder="drop",
    sparse_threshold=1.0  # mantenemos salida esparza por el bloque texto
)

# Modelo
model = LogisticRegression(max_iter=1000, class_weight="balanced")  # balanced por si hay desbalance
pipe = Pipeline([
    ("prep", preprocessor),
    ("clf",  model)
])

# X e y finales
X = tweets[["clean_text"] + num_cols + obj_cols]  # el CT tomará lo que necesita
y = tweets["bots"].astype(int)

print("Clases en y:", dict(y.value_counts()))

```


## Split (división), entrenamiento y evaluación
Aprende de los tweets (vocabulario) y del ser caso elimina las stop-words
```{python}

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec  = recall_score(y_test, y_pred, zero_division=0)
f1   = f1_score(y_test, y_pred, zero_division=0)

print(f"accuracy:  {acc:.3f}")
print(f"precision: {prec:.3f}")
print(f"recall:    {rec:.3f}")
print(f"f1:        {f1:.3f}")

print("\nClassification report:")
print(classification_report(y_test, y_pred, zero_division=0))


```

## Matriz de confusión
```{python}

ConfusionMatrixDisplay.from_predictions(y_test, y_pred)

```

## Inspección: palabras más comunes (TF-IDF)

```{python}
# Top términos por TF-IDF medio (en el train)
from scipy.sparse import issparse

X_train_text = pipe.named_steps["prep"].named_transformers_["text"].transform(X_train["clean_text"])
idf_vocab    = pipe.named_steps["prep"].named_transformers_["text"].get_feature_names_out()

# Promedio por columna
if issparse(X_train_text):
    col_means = np.array(X_train_text.mean(axis=0)).ravel()
else:
    col_means = X_train_text.mean(axis=0)

top_idx = np.argsort(col_means)[::-1][:20]
top_terms = [(idf_vocab[i], float(col_means[i])) for i in top_idx]
pd.DataFrame(top_terms, columns=["term","mean_tfidf"])


```
