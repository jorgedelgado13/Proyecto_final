---
title: "Análisis ML de Toxicidad en Tweets (Ecuador)"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
execute:
  echo: true
  warning: false
  message: false
---

## 0. Preparación

```{python}
import pandas as pd
import numpy as np
import re, ast
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize # tokenizacion
from nltk import pos_tag #lematizacion
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from wordcloud import WordCloud
from collections import Counter

nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)
stop_es = stopwords.words("spanish")

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    classification_report, ConfusionMatrixDisplay,
    mean_absolute_error, mean_squared_error, r2_score,
    silhouette_score
)

from sklearn.cluster import KMeans

# Ruta al CSV (deja el archivo en la misma carpeta que este .qmd)
df = pd.read_csv("data/1500_tweets_con_toxicity.csv", encoding="utf-8")
df.head()
```


## 1. EDA

```{python}
df.info()
```
```{python}
# Detectar tipos de columnas
num_vars = df.select_dtypes(include=['number']).columns
cat_vars = df.select_dtypes(include=['object', 'category']).columns
text_vars = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 30]

# Mostrar resultados
print("Tipos de variables en el DataFrame:\n")
print(f"Variables numéricas ({len(num_vars)}): {list(num_vars)}")
print(f"Variables categóricas ({len(cat_vars)}): {list(cat_vars)}")
print(f"Variables de texto ({len(text_vars)}): {list(text_vars)}")

# Resumen tabular:
resumen = pd.DataFrame({
    'Tipo': ['Numéricas', 'Categóricas', 'Texto'],
    'Cantidad': [len(num_vars), len(cat_vars), len(text_vars)]
})

display(resumen)
```

```{python}
# Resumen numérico
df.describe(include="number").T
```

```{python}
num_df = df.select_dtypes(include=['number'])
# Distribución (histogramas)
print("\n Distribuciones de variables numéricas:")
num_df.hist(bins=20, figsize=(12, 8), edgecolor='black')
plt.suptitle("Distribución de variables numéricas", fontsize=14)
plt.tight_layout()
plt.show()
```


```{python}
# Resumen categórico
df.select_dtypes(exclude="number").nunique().sort_values(ascending=False).head(20)
```


```{python}
# Nulos y duplicados
missing = df.isna().sum().sort_values(ascending=False).to_frame("n_missing")
missing["pct"] = (df.isna().mean()*100).sort_values(ascending=False)
missing.head(10)
```

```{python}
# Distribución toxicity_score
df["toxicity_score"] = pd.to_numeric(df["toxicity_score"], errors="coerce")
df["toxicity_score"].dropna().hist(bins=30)
plt.title("Distribución de toxicity_score")
plt.xlabel("toxicity_score"); plt.ylabel("Frecuencia"); plt.show()

df["toxicity_score"].describe(percentiles=[0.1,0.25,0.5,0.75,0.9,0.95])
```


```{python}
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
sns.histplot(df['toxicity_score'], bins=30, kde=True, color='steelblue')
plt.title('Distribución de toxicity_score')
plt.xlabel('toxicity_score')
plt.ylabel('Frecuencia')
plt.tight_layout()
plt.show()
```


```{python}
# Boxplots de numéricas clave
num_cols = ["authorFollowers", "time_response", "account_age_days", "mentions_count", "hashtags_count", "content_length", "sentiment_polarity"]
present = [c for c in num_cols if c in df.columns]
df[present].boxplot(rot=45); plt.title("Boxplots numéricos"); plt.tight_layout(); plt.show()
```

```{python}
# Top hashtags y menciones
def parse_list_like(x):
    if pd.isna(x): return []
    s = str(x).strip()
    try:
        obj = ast.literal_eval(s)
        if isinstance(obj, list):
            return [str(t).strip().lower() for t in obj if str(t).strip()]
    except Exception:
        pass
    s = re.sub(r"[\[\]{}()'\"#]", " ", s)
    tokens = re.split(r"[,\s]+", s)
    return [t.strip().lower() for t in tokens if t.strip()]

for col, title in [("hashtags","Top hashtags"), ("mentions","Top menciones")]:
    if col in df.columns:
        items = []
        for v in df[col]:
            items.extend(parse_list_like(v))
        if items:
            top = pd.Series(items).value_counts().head(20)
            top.sort_values(ascending=True).plot(kind="barh"); plt.title(title); plt.tight_layout(); plt.show()
            display(top.to_frame("count"))
```

```{python}
# Conteo de Variables categóricas

cat_cols = df.select_dtypes(include=['object', 'category']).columns

if len(cat_cols) > 0:
    for col in cat_cols[:5]:  # graficar hasta 5 variables categóricas
        # Contar categorías y tomar solo las 10 principales
        top_cats = df[col].value_counts().head(10)

        plt.figure(figsize=(8, 4))
        sns.barplot(y=top_cats.index, x=top_cats.values, palette='viridis')
        plt.title(f"Top 10 categorías más frecuentes en: {col}")
        plt.xlabel("Frecuencia")
        plt.ylabel(col)
        plt.tight_layout()
        plt.show()
else:
    print("No hay variables categóricas para graficar.\n")
```


```{python}
# Detectar variable de texto
text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 30]

if len(text_cols) > 0:
    col_texto = text_cols[1]  # usa la segunda columna tipo texto
    texto = " ".join(df[col_texto].dropna().astype(str))

    # --- Generar nube de palabras ---
    plt.figure(figsize=(10,6))
    wc = WordCloud(width=800, height=400, background_color='white', colormap='viridis', max_words=100).generate(texto)
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Nube de palabras - {col_texto}")
    plt.show()

    # --- Calcular las 10 palabras más frecuentes ---
    palabras = texto.split()
    frec = Counter(palabras)
    comunes = pd.DataFrame(frec.most_common(10), columns=['Palabra', 'Frecuencia'])

    # --- Gráfico Top 10 ---
    plt.figure(figsize=(8,4))
    sns.barplot(data=comunes, x='Frecuencia', y='Palabra', palette='mako')
    plt.title(f"Top 10 palabras más representativas - {col_texto}")
    plt.tight_layout()
    plt.show()

    # --- Mostrar tabla ---
    display(comunes)
else:
    print("No se detectó ninguna columna de texto para generar la nube de palabras.\n")
```

```{python}
#Correlación de las variables numéricas
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if len(numeric_cols) > 1:
    # Matriz de correlación
    correlation_matrix = df[numeric_cols].corr()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, fmt='.2f')
    plt.title('Matriz de Correlación de Variables Numéricas')
    plt.tight_layout()
    plt.show()
    
    # Top correlaciones con toxicidad
    if 'toxicity_score' in numeric_cols:
        toxicity_corr = correlation_matrix['toxicity_score'].abs().sort_values(ascending=False)
        print(f"=== CORRELACIONES CON {'toxicity_score'.upper()} ===")
        print(toxicity_corr.head(10))
```


**Hallazgos clave (EDA):**  
```{python}
print("DISTRIBUCIÓN DE LOS DATOS:")
print(f"- Mi Set de datos tiene {df.shape[0]} observaciones y {df.shape[1]} variables")
print(f"- Variables numéricas son: {len(numeric_cols)}")
print(f"- Variables categóricas son: {len(cat_cols)}")

print(f"VARIABLE OBJETIVO (Score de TOXICIDAD):")
toxicity_mean = df['toxicity_score'].mean()
toxicity_std = df['toxicity_score'].std()
print(f"- Media de toxicidad: {toxicity_mean:.4f}")
print(f"- Desviación estándar: {toxicity_std:.4f}")
print(f"- La mayoría de tweets tienen baja toxicidad pues son menores a 0.5, la toxicidad media es: {toxicity_mean:.4f}")

print(f"Distribución de `toxicity_score` con mediana, cuartiles y % de valores altos.")
print(f"Campos con nulos relevantes (p.ej. `toxicity_score` y `hashtags`).")
print(f"Concentración de polaridad y relación preliminar con la toxicidad.")
```


## 2. Preprocesamiento y codificación
```{python}
df_procesado = df[['content', 'toxicity_score']].copy()

initial_rows = len(df_procesado)
df_procesado = df_procesado.dropna(subset=['toxicity_score', 'content'])
final_rows = len(df_procesado)

# Función simple de limpieza de texto
def clean_text(text):
    if pd.isna(text):
        return ""
    
    text = str(text).lower()
    # Remover URLs, menciones, hashtags, además números y caracteres especiales
    text = re.sub(r'http\S+|www\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = ' '.join(text.split())
    
    return text

# Aplicar limpieza
df_procesado['content_clean'] = df_procesado['content'].apply(clean_text)

# Remover stopwords en español
spanish_stopwords = set(stopwords.words('spanish'))

def remove_stopwords(text):
    words = word_tokenize(text, language='spanish')  # <-- cambio: antes text.split()
    return ' '.join([word for word in words if word.lower() not in spanish_stopwords and len(word) > 2])

# Crear texto procesado final
df_procesado['text_processed'] = df_procesado['content_clean'].apply(remove_stopwords)
print(df_procesado)
```

```{python}
text_col = "text_processed"
num_cols = ["authorFollowers", "account_age_days", "mentions_count", "hashtags_count", "content_length", "sentiment_polarity"]
cat_cols = ["source", "has_profile_picture"]

# Subconjunto para modelado
df_model = df[["content", "toxicity_score"] + num_cols + cat_cols].copy()
for c in num_cols:
    df_model[c] = pd.to_numeric(df_model[c], errors="coerce")
for c in cat_cols:
    df_model[c] = df_model[c].astype("category")
df_model["toxicity_score"] = pd.to_numeric(df_model["toxicity_score"], errors="coerce")

# Añadir la columna de texto procesado alineando por 'content'
df_model = df_model.merge(df_procesado[["content", "text_processed"]], on="content", how="left")

# Filtrar NAs en texto/target
df_model = df_model.dropna(subset=[text_col, "toxicity_score"]).reset_index(drop=True)

X = df_model[[text_col] + num_cols + cat_cols]
y_cont = df_model["toxicity_score"]

preprocessor = ColumnTransformer(
    transformers=[
        ("text", TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2), strip_accents="unicode"), text_col),
        ("num", Pipeline([("scaler", StandardScaler(with_mean=False))]), num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
    ]
)
preprocessor
```

## 3. Clasificación

```{python}
# Candidatos de umbral 
candidate_thresholds = np.array([0.30, 0.40, 0.50, 0.60, 0.70])

# Tabla de balance por umbral elección de target binario
balance_rows = []
for t in candidate_thresholds:
    y_tmp = (y_cont >= t).astype(int)
    pos = int(y_tmp.sum())
    neg = int((1 - y_tmp).sum())
    pos_pct = float(y_tmp.mean()) * 100.0
    balance_rows.append({"threshold": t, "positivos": pos, "negativos": neg, "pct_positivos": round(pos_pct, 2)})
balance_df = pd.DataFrame(balance_rows).sort_values("threshold")
print("Balance por umbral :")
print(balance_df.to_string(index=False))

# Criterio simple de selección: tasa objetivo de positivos ≈ 20%
target_pos_rate = 0.20
best_threshold = min(candidate_thresholds, key=lambda t: abs(((y_cont >= t).mean()) - target_pos_rate))
print(f"\nUmbral sugerido por balance de clases (~{int(target_pos_rate*100)}% positivos): {best_threshold:.2f}")

# -----------------------------------------------------------------
# Target binario usando el umbral seleccionado
y_bin = (y_cont >= best_threshold).astype(int)

# Split y entrenamiento
X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2, random_state=42, stratify=y_bin)

pipe_clf = Pipeline([("preprocess", preprocessor),
                     ("clf", RandomForestClassifier(
                         n_estimators=300,
                         random_state=42))])

pipe_clf.fit(X_train, y_train)
y_pred = pipe_clf.predict(X_test)
y_proba = pipe_clf.predict_proba(X_test)[:, 1]

metrics = {
    "threshold_usado": float(best_threshold),
    "accuracy": accuracy_score(y_test, y_pred),
    "precision": precision_score(y_test, y_pred, zero_division=0),
    "recall": recall_score(y_test, y_pred, zero_division=0),
    "f1": f1_score(y_test, y_pred, zero_division=0),
    "roc_auc": roc_auc_score(y_test, y_proba),
}
metrics
```

```{python}
# Target binario con umbral 0.5 solo para el split (estratificación estable)
y_bin_split = (y_cont >= 0.5).astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y_bin_split, test_size=0.2, random_state=42, stratify=y_bin_split)

from sklearn.ensemble import RandomForestClassifier

pipe_clf = Pipeline([("preprocess", preprocessor),
                     ("clf", RandomForestClassifier(
                         n_estimators=300,
                         random_state=42))])

pipe_clf.fit(X_train, y_train)

# Probabilidades en test
y_proba = pipe_clf.predict_proba(X_test)[:, 1]

# --- Búsqueda de umbral por F1 ---
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

thresholds = np.linspace(0.20, 0.80, 61)  # explora 0.20..0.80
rows = []
for t in thresholds:
    y_pred_t = (y_proba >= t).astype(int)
    rows.append({
        "threshold": float(t),
        "accuracy": accuracy_score(y_test, y_pred_t),
        "precision": precision_score(y_test, y_pred_t, zero_division=0),
        "recall": recall_score(y_test, y_pred_t, zero_division=0),
        "f1": f1_score(y_test, y_pred_t, zero_division=0)
    })

import pandas as pd
thr_df = pd.DataFrame(rows)
best_row = thr_df.loc[thr_df["f1"].idxmax()]
best_threshold = float(best_row["threshold"])

# Métricas con umbral óptimo (F1) vs 0.5
y_pred_best = (y_proba >= best_threshold).astype(int)
y_pred_05   = (y_proba >= 0.5).astype(int)

metrics_best = {
    "threshold_usado": best_threshold,
    "accuracy": accuracy_score(y_test, y_pred_best),
    "precision": precision_score(y_test, y_pred_best, zero_division=0),
    "recall": recall_score(y_test, y_pred_best, zero_division=0),
    "f1": f1_score(y_test, y_pred_best, zero_division=0),
    "roc_auc": roc_auc_score(y_test, y_proba)  # AUC no depende del umbral
}

metrics_05 = {
    "threshold_usado": 0.5,
    "accuracy": accuracy_score(y_test, y_pred_05),
    "precision": precision_score(y_test, y_pred_05, zero_division=0),
    "recall": recall_score(y_test, y_pred_05, zero_division=0),
    "f1": f1_score(y_test, y_pred_05, zero_division=0),
    "roc_auc": roc_auc_score(y_test, y_proba)
}

print("Métricas con umbral óptimo (F1):", metrics_best)
print("Métricas con umbral 0.5:", metrics_05)
```

```{python}
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
plt.title("Matriz de confusión - Clasificación"); plt.tight_layout(); plt.show()
```

## 4. Regresión

```{python}
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_cont, test_size=0.2, random_state=42)

# Baseline: LinearRegression
pipe_reg_lr = Pipeline([("preprocess", preprocessor), ("reg", LinearRegression())])
pipe_reg_lr.fit(X_train_r, y_train_r)
y_pred_lr = pipe_reg_lr.predict(X_test_r)

reg_metrics_lr = {
    "MAE": mean_absolute_error(y_test_r, y_pred_lr),
    "RMSE": float(np.sqrt(mean_squared_error(y_test_r, y_pred_lr))),  # RMSE correcto
    "R2": r2_score(y_test_r, y_pred_lr),
}
print({"LinearRegression": reg_metrics_lr})

# Mejora: Ridge con búsqueda simple de alpha para comparar mejor modelo para usar entre Regresión Lineal y Ridge
pipe_reg_ridge = Pipeline([("preprocess", preprocessor), ("reg", Ridge())])
param_grid = {"reg__alpha": [0.1, 1.0, 3.0, 10.0]}

try:
    gs = GridSearchCV(
        pipe_reg_ridge,
        param_grid=param_grid,
        scoring="neg_mean_absolute_error",
        cv=3,                # menos folds para reducir costo
        n_jobs=1,            # sin paralelismo (evita TerminatedWorkerError)
        pre_dispatch="1*n_jobs",
        verbose=0
    )
    gs.fit(X_train_r, y_train_r)
    y_pred_ridge = gs.predict(X_test_r)

    reg_metrics_ridge = {
        "MAE": mean_absolute_error(y_test_r, y_pred_ridge),
        "RMSE": float(np.sqrt(mean_squared_error(y_test_r, y_pred_ridge))),
        "R2": r2_score(y_test_r, y_pred_ridge),
    }
    print({"Ridge": {"best_alpha": gs.best_params_["reg__alpha"], **reg_metrics_ridge}})

except Exception as e:
    print(f"[Aviso] GridSearchCV falló: {e}\nSe usará Ridge(alpha=1.0) sin CV.")
    pipe_reg_ridge = Pipeline([("preprocess", preprocessor), ("reg", Ridge(alpha=1.0))])
    pipe_reg_ridge.fit(X_train_r, y_train_r)
    y_pred_ridge = pipe_reg_ridge.predict(X_test_r)

    reg_metrics_ridge = {
        "MAE": mean_absolute_error(y_test_r, y_pred_ridge),
        "RMSE": float(np.sqrt(mean_squared_error(y_test_r, y_pred_ridge))),
        "R2": r2_score(y_test_r, y_pred_ridge),
    }
    print({"Ridge": {"best_alpha": 1.0, **reg_metrics_ridge}})

# Elegimos el mejor por RMSE
best_name = "Ridge" if reg_metrics_ridge["RMSE"] < reg_metrics_lr["RMSE"] else "LinearRegression"
y_pred_r = y_pred_ridge if best_name == "Ridge" else y_pred_lr
print({"mejor_modelo": best_name})
```

```{python}
# Dispersión Real vs Predicho con línea ideal y=x
plt.figure()
plt.scatter(y_test_r, y_pred_r, alpha=0.6)
xs = np.linspace(0, 1, 100)
plt.plot(xs, xs, linestyle="--")  # línea ideal
plt.title("Real vs. Predicho (Regresión)")
plt.xlabel("Real"); plt.ylabel("Predicho"); plt.tight_layout(); plt.show()

# Residuales
res = y_test_r - y_pred_r
plt.figure()
plt.scatter(y_pred_r, res, alpha=0.6)
plt.axhline(0, linestyle="--")
plt.title("Residuales vs. Predicción")
plt.xlabel("Predicción"); plt.ylabel("Residual"); plt.tight_layout(); plt.show()
```

## 5. Clustering

```{python}
num_cols_cluster = ["authorFollowers", "account_age_days", "mentions_count", "hashtags_count", "content_length", "sentiment_polarity", "toxicity_score"]
dfc = df[num_cols_cluster].apply(pd.to_numeric, errors="coerce").dropna().reset_index(drop=True)

sil_scores = {}
best = {"k": None, "score": -1, "labels": None}

for k in range(2,7):
    pipe_km = Pipeline([("scaler", StandardScaler()), ("km", KMeans(n_clusters=k, n_init=10, random_state=42))])
    labels = pipe_km.fit_predict(dfc)
    sil = silhouette_score(dfc, labels)
    sil_scores[k] = sil
    if sil > best["score"]:
        best = {"k": k, "score": sil, "labels": labels, "model": pipe_km}

sil_scores, best["k"], best["score"]
```

```{python}
dfvis = dfc.copy()
dfvis["cluster"] = best["labels"]
for cl in sorted(dfvis["cluster"].unique()):
    part = dfvis[dfvis["cluster"]==cl]
    plt.scatter(part["sentiment_polarity"], part["toxicity_score"], alpha=0.6, label=f"Cluster {cl}")
plt.legend(); plt.title(f"Clusters KMeans (k={best['k']})"); plt.xlabel("sentiment_polarity"); plt.ylabel("toxicity_score"); plt.tight_layout(); plt.show()

# Relación con target binario (0.5)
y_bin_all = (dfvis["toxicity_score"] >= 0.5).astype(int)
ct = pd.crosstab(dfvis["cluster"], y_bin_all, normalize="index")*100
ct.round(1)
```

## 6. Conclusiones y próximos pasos

- Calidad de datos y nulos: …  
- Justificación del umbral para clasificación: …  
- Rendimiento de modelos: …  
- Patrones en clusters vs clases: …  

**Mejoras propuestas:**
- Probar `Ridge` o `ElasticNet` para regresión.
- Ajustar umbrales y `class_weight` en clasificación; búsqueda de hiperparámetros con `GridSearchCV`.
- Expandir features del texto: `ngram_range`, `min_df`, limpieza de URLs, usuarios y emojis.
- Evaluar reducción de dimensionalidad para clustering/visualización (PCA/TruncatedSVD).