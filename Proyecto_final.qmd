---
title: "Análisis ML de Toxicidad en Tweets (Ecuador)"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
execute:
  echo: true
  warning: false
  message: false
---

## 0. Preparación

```{python}
import pandas as pd
import numpy as np
import re, ast
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize # tokenizacion
from nltk import pos_tag #lematizacion
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from wordcloud import WordCloud
from collections import Counter

nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)
stop_es = stopwords.words("spanish")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    classification_report, ConfusionMatrixDisplay,
    mean_absolute_error, mean_squared_error, r2_score,
    silhouette_score
)

from sklearn.cluster import KMeans

# Ruta al CSV (deja el archivo en la misma carpeta que este .qmd)
df = pd.read_csv("data/1500_tweets_con_toxicity.csv", encoding="utf-8")
df.head()
```


## 1. EDA

```{python}
df.info()
```
```{python}
# Detectar tipos de columnas
num_vars = df.select_dtypes(include=['number']).columns
cat_vars = df.select_dtypes(include=['object', 'category']).columns
text_vars = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 30]

# Mostrar resultados
print("Tipos de variables en el DataFrame:\n")
print(f"Variables numéricas ({len(num_vars)}): {list(num_vars)}")
print(f"Variables categóricas ({len(cat_vars)}): {list(cat_vars)}")
print(f"Variables de texto ({len(text_vars)}): {list(text_vars)}")

# Resumen tabular:
resumen = pd.DataFrame({
    'Tipo': ['Numéricas', 'Categóricas', 'Texto'],
    'Cantidad': [len(num_vars), len(cat_vars), len(text_vars)]
})

display(resumen)
```

```{python}
# Resumen numérico
df.describe(include="number").T
```

```{python}
num_df = df.select_dtypes(include=['number'])
# Distribución (histogramas)
print("\n Distribuciones de variables numéricas:")
num_df.hist(bins=20, figsize=(12, 8), edgecolor='black')
plt.suptitle("Distribución de variables numéricas", fontsize=14)
plt.tight_layout()
plt.show()
```


```{python}
# Resumen categórico
df.select_dtypes(exclude="number").nunique().sort_values(ascending=False).head(20)
```


```{python}
# Nulos y duplicados
missing = df.isna().sum().sort_values(ascending=False).to_frame("n_missing")
missing["pct"] = (df.isna().mean()*100).sort_values(ascending=False)
missing.head(10)
```

```{python}
# Distribución toxicity_score
df["toxicity_score"] = pd.to_numeric(df["toxicity_score"], errors="coerce")
df["toxicity_score"].dropna().hist(bins=30)
plt.title("Distribución de toxicity_score")
plt.xlabel("toxicity_score"); plt.ylabel("Frecuencia"); plt.show()

df["toxicity_score"].describe(percentiles=[0.1,0.25,0.5,0.75,0.9,0.95])
```


```{python}
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
sns.histplot(df['toxicity_score'], bins=30, kde=True, color='steelblue')
plt.title('Distribución de toxicity_score')
plt.xlabel('toxicity_score')
plt.ylabel('Frecuencia')
plt.tight_layout()
plt.show()
```


```{python}
# Boxplots de numéricas clave
num_cols = ["authorFollowers", "time_response", "account_age_days", "mentions_count", "hashtags_count", "content_length", "sentiment_polarity"]
present = [c for c in num_cols if c in df.columns]
df[present].boxplot(rot=45); plt.title("Boxplots numéricos"); plt.tight_layout(); plt.show()
```

```{python}
# Top hashtags y menciones
def parse_list_like(x):
    if pd.isna(x): return []
    s = str(x).strip()
    try:
        obj = ast.literal_eval(s)
        if isinstance(obj, list):
            return [str(t).strip().lower() for t in obj if str(t).strip()]
    except Exception:
        pass
    s = re.sub(r"[\[\]{}()'\"#]", " ", s)
    tokens = re.split(r"[,\s]+", s)
    return [t.strip().lower() for t in tokens if t.strip()]

for col, title in [("hashtags","Top hashtags"), ("mentions","Top menciones")]:
    if col in df.columns:
        items = []
        for v in df[col]:
            items.extend(parse_list_like(v))
        if items:
            top = pd.Series(items).value_counts().head(20)
            top.sort_values(ascending=True).plot(kind="barh"); plt.title(title); plt.tight_layout(); plt.show()
            display(top.to_frame("count"))
```

```{python}
# Conteo de Variables categóricas

cat_cols = df.select_dtypes(include=['object', 'category']).columns

if len(cat_cols) > 0:
    for col in cat_cols[:5]:  # graficar hasta 5 variables categóricas
        # Contar categorías y tomar solo las 10 principales
        top_cats = df[col].value_counts().head(10)

        plt.figure(figsize=(8, 4))
        sns.barplot(y=top_cats.index, x=top_cats.values, palette='viridis')
        plt.title(f"Top 10 categorías más frecuentes en: {col}")
        plt.xlabel("Frecuencia")
        plt.ylabel(col)
        plt.tight_layout()
        plt.show()
else:
    print("No hay variables categóricas para graficar.\n")
```


```{python}
# Detectar variable de texto
text_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.len().mean() > 30]

if len(text_cols) > 0:
    col_texto = text_cols[1]  # usa la segunda columna tipo texto
    texto = " ".join(df[col_texto].dropna().astype(str))

    # --- Generar nube de palabras ---
    plt.figure(figsize=(10,6))
    wc = WordCloud(width=800, height=400, background_color='white', colormap='viridis', max_words=100).generate(texto)
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Nube de palabras - {col_texto}")
    plt.show()

    # --- Calcular las 10 palabras más frecuentes ---
    palabras = texto.split()
    frec = Counter(palabras)
    comunes = pd.DataFrame(frec.most_common(10), columns=['Palabra', 'Frecuencia'])

    # --- Gráfico Top 10 ---
    plt.figure(figsize=(8,4))
    sns.barplot(data=comunes, x='Frecuencia', y='Palabra', palette='mako')
    plt.title(f"Top 10 palabras más representativas - {col_texto}")
    plt.tight_layout()
    plt.show()

    # --- Mostrar tabla ---
    display(comunes)
else:
    print("No se detectó ninguna columna de texto para generar la nube de palabras.\n")
```

```{python}
#Correlación de las variables numéricas
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if len(numeric_cols) > 1:
    # Matriz de correlación
    correlation_matrix = df[numeric_cols].corr()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, fmt='.2f')
    plt.title('Matriz de Correlación de Variables Numéricas')
    plt.tight_layout()
    plt.show()
    
    # Top correlaciones con toxicidad
    if 'toxicity_score' in numeric_cols:
        toxicity_corr = correlation_matrix['toxicity_score'].abs().sort_values(ascending=False)
        print(f"=== CORRELACIONES CON {'toxicity_score'.upper()} ===")
        print(toxicity_corr.head(10))
```


**Hallazgos clave (EDA):**  
```{python}
print("DISTRIBUCIÓN DE LOS DATOS:")
print(f"- Mi Set de datos tiene {df.shape[0]} observaciones y {df.shape[1]} variables")
print(f"- Variables numéricas son: {len(numeric_cols)}")
print(f"- Variables categóricas son: {len(cat_cols)}")

print(f"VARIABLE OBJETIVO (Score de TOXICIDAD):")
toxicity_mean = df['toxicity_score'].mean()
toxicity_std = df['toxicity_score'].std()
print(f"- Media de toxicidad: {toxicity_mean:.4f}")
print(f"- Desviación estándar: {toxicity_std:.4f}")
print(f"- La mayoría de tweets tienen baja toxicidad pues son menores a 0.5, la toxicidad media es: {toxicity_mean:.4f}")

print(f"Distribución de `toxicity_score` con mediana, cuartiles y % de valores altos.")
print(f"Campos con nulos relevantes (p.ej. `toxicity_score` y `hashtags`).")
print(f"Concentración de polaridad y relación preliminar con la toxicidad.")
```


## 2. Preprocesamiento y codificación
```{python}
df_procesado = df[['content', 'toxicity_score']].copy()

initial_rows = len(df_procesado)
df_procesado = df_procesado.dropna(subset=['toxicity_score', 'content'])
final_rows = len(df_procesado)

# Función simple de limpieza de texto
def clean_text(text):
    if pd.isna(text):
        return ""
    
    text = str(text).lower()
    # Remover URLs, menciones, hashtags, ademas números y caracteres especiales
    text = re.sub(r'http\S+|www\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = ' '.join(text.split())
    
    return text

# Aplicar limpieza
df_procesado['content_clean'] = df_procesado['content'].apply(clean_text)

# Remover stopwords en español
spanish_stopwords = set(stopwords.words('spanish'))

def remove_stopwords(text):
    words = text.split()
    return ' '.join([word for word in words if word not in spanish_stopwords and len(word) > 2])

# Crear texto procesado final
df_procesado['text_processed'] = df_procesado['content_clean'].apply(remove_stopwords)
print(df_procesado)
```

```{python}
text_col = "text_processed"
num_cols = ["authorFollowers", "account_age_days", "mentions_count", "hashtags_count", "content_length", "sentiment_polarity"]
cat_cols = ["source", "has_profile_picture"]

# Subconjunto para modelado
df_model = df[["content", "toxicity_score"] + num_cols + cat_cols].copy()
for c in num_cols:
    df_model[c] = pd.to_numeric(df_model[c], errors="coerce")
for c in cat_cols:
    df_model[c] = df_model[c].astype("category")
df_model["toxicity_score"] = pd.to_numeric(df_model["toxicity_score"], errors="coerce")

# Añadir la columna de texto procesado alineando por 'content'
df_model = df_model.merge(df_procesado[["content", "text_processed"]], on="content", how="left")

# Filtrar NAs en texto/target
df_model = df_model.dropna(subset=[text_col, "toxicity_score"]).reset_index(drop=True)

X = df_model[[text_col] + num_cols + cat_cols]
y_cont = df_model["toxicity_score"]

preprocessor = ColumnTransformer(
    transformers=[
        ("text", TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2), strip_accents="unicode"), text_col),
        ("num", Pipeline([("scaler", StandardScaler(with_mean=False))]), num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
    ]
)
preprocessor
```

## 3. Clasificación

```{python}
# Target binario con umbral 0.5 (puedes justificar 0.5 vs 0.7 en el informe)
y_bin = (y_cont >= 0.5).astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2, random_state=42, stratify=y_bin)

pipe_clf = Pipeline([("preprocess", preprocessor),
                     ("clf", LogisticRegression(max_iter=300, solver="liblinear", class_weight="balanced"))])

pipe_clf.fit(X_train, y_train)
y_pred = pipe_clf.predict(X_test)
y_proba = pipe_clf.predict_proba(X_test)[:,1]

metrics = {
    "accuracy": accuracy_score(y_test, y_pred),
    "precision": precision_score(y_test, y_pred, zero_division=0),
    "recall": recall_score(y_test, y_pred, zero_division=0),
    "f1": f1_score(y_test, y_pred, zero_division=0),
    "roc_auc": roc_auc_score(y_test, y_proba),
}
metrics
```

```{python}
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
plt.title("Matriz de confusión - Clasificación"); plt.tight_layout(); plt.show()
```

## 4. Regresión

```{python}
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_cont, test_size=0.2, random_state=42)

pipe_reg = Pipeline([("preprocess", preprocessor), ("reg", LinearRegression())])
pipe_reg.fit(X_train_r, y_train_r)
y_pred_r = pipe_reg.predict(X_test_r)

reg_metrics = {
    "MAE": mean_absolute_error(y_test_r, y_pred_r),
    "RMSE": mean_squared_error(y_test_r, y_pred_r),
    "R2": r2_score(y_test_r, y_pred_r),
}
print(reg_metrics)
```

```{python}
plt.scatter(y_test_r, y_pred_r, alpha=0.6)
plt.title("Real vs. Predicho (Regresión)"); plt.xlabel("Real"); plt.ylabel("Predicho"); plt.tight_layout(); plt.show()

res = y_test_r - y_pred_r
plt.scatter(y_pred_r, res, alpha=0.6); plt.axhline(0, linestyle="--")
plt.title("Residuales vs. Predicción"); plt.xlabel("Predicción"); plt.ylabel("Residual"); plt.tight_layout(); plt.show()
```

## 5. Clustering

```{python}
num_cols_cluster = ["authorFollowers", "account_age_days", "mentions_count", "hashtags_count", "content_length", "sentiment_polarity", "toxicity_score"]
dfc = df[num_cols_cluster].apply(pd.to_numeric, errors="coerce").dropna().reset_index(drop=True)

sil_scores = {}
best = {"k": None, "score": -1, "labels": None}

for k in range(2,7):
    pipe_km = Pipeline([("scaler", StandardScaler()), ("km", KMeans(n_clusters=k, n_init=10, random_state=42))])
    labels = pipe_km.fit_predict(dfc)
    sil = silhouette_score(dfc, labels)
    sil_scores[k] = sil
    if sil > best["score"]:
        best = {"k": k, "score": sil, "labels": labels, "model": pipe_km}

sil_scores, best["k"], best["score"]
```

```{python}
dfvis = dfc.copy()
dfvis["cluster"] = best["labels"]
for cl in sorted(dfvis["cluster"].unique()):
    part = dfvis[dfvis["cluster"]==cl]
    plt.scatter(part["sentiment_polarity"], part["toxicity_score"], alpha=0.6, label=f"Cluster {cl}")
plt.legend(); plt.title(f"Clusters KMeans (k={best['k']})"); plt.xlabel("sentiment_polarity"); plt.ylabel("toxicity_score"); plt.tight_layout(); plt.show()

# Relación con target binario (0.5)
y_bin_all = (dfvis["toxicity_score"] >= 0.5).astype(int)
ct = pd.crosstab(dfvis["cluster"], y_bin_all, normalize="index")*100
ct.round(1)
```

## 6. Conclusiones y próximos pasos

- Calidad de datos y nulos: …  
- Justificación del umbral para clasificación: …  
- Rendimiento de modelos: …  
- Patrones en clusters vs clases: …  

**Mejoras propuestas:**
- Probar `Ridge` o `ElasticNet` para regresión.
- Ajustar umbrales y `class_weight` en clasificación; búsqueda de hiperparámetros con `GridSearchCV`.
- Expandir features del texto: `ngram_range`, `min_df`, limpieza de URLs, usuarios y emojis.
- Evaluar reducción de dimensionalidad para clustering/visualización (PCA/TruncatedSVD).