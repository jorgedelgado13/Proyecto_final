---
title: "Laboratorio"
code-fold: false
---

# Importar librerías

```{python}
# ====== Importar librerías ======
import re
import unicodedata
import pandas as pd
import altair as alt

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# NLTK
import nltk
from nltk.corpus import stopwords
    

# Descargas necesarias de NLTK (solo se descargan una vez)
nltk.download("stopwords")
stop_es = stopwords.words("spanish")
# ====== Cargar data ======
tweets = pd.read_csv("data/tweets_totales_con_sentimiento_ml.csv", encoding="utf-8")

# Muestra básica
display(tweets.head())
tweets.info()
print("shape:", tweets.shape)
print(tweets.dtypes)

```


# Cargar data

```{python}
tweets = pd.read_csv("data/tweets_totales_con_sentimiento_ml.csv")

# Muestra las primeras filas
print(tweets.head)
print(tweets.info)
print(tweets.shape)
print(tweets.dtypes)
```
Entender la estructura del dataset nos da una mejor idea de con que estamos lidiando

### Porcentaje de Missing Values por Columna

```{python}
# ====== Porcentaje de Missing Values por Columna ======
nan_percent = tweets.isna().mean() * 100
nan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)
nan_percent_sorted

```


# Llenar celdas vacias 
```{python}
texts_raw = tweets["content"].fillna("").astype(str)
```

    
## Función de limpieza

```{python}
# ====== Función de limpieza (conserva 'ñ', quita tildes) ======
# En lugar de NFD (que rompería la ñ), mapeamos tildes explícitamente.
_TILDE_MAP = str.maketrans({
    "á": "a", "é": "e", "í": "i", "ó": "o", "ú": "u",
    "Á": "a", "É": "e", "Í": "i", "Ó": "o", "Ú": "u",
    "ü": "u", "Ü": "u"
})

def normalize_text(text: str) -> str:
    text = str(text).lower().strip()
    # URLs
    text = re.sub(r"https?://\S+|www\.\S+", " ", text)
    # menciones
    text = re.sub(r"@\w+", " ", text)
    # hashtags (si no te sirven)
    text = re.sub(r"#\w+", " ", text)
    # quitar tildes pero NO tocar 'ñ'
    text = text.translate(_TILDE_MAP)
    # conservar letras (a-z, ñ) y espacios
    text = re.sub(r"[^a-zñ\s]", " ", text)
    # colapsar espacios
    text = re.sub(r"\s+", " ", text).strip()
    return text

tweets["clean_text"] = texts_raw.apply(normalize_text)
tweets[["content", "clean_text"]].head()
```


## Fit_transform
Aprende de los tweets (vocabulario) y del ser caso elimina las stop-words
```{python}
stop_es = stopwords.words("spanish")
vectorizer = TfidfVectorizer(stop_words=stop_es)
#vectorizer = CountVectorizer(stop_words=stop_es) 

```


```{python}

X = vectorizer.fit_transform(tweets["clean_text"])
```

## Obtener las palabras finales

```{python}
print("Matriz X:", X.shape)      # (n_docs, n_terms)
vocab = vectorizer.get_feature_names_out()
print("Primeras 20 palabras:", vocab[:20])

sample_rows = min(10, X.shape[0])
preview_df = pd.DataFrame(
    X[:sample_rows].toarray(),
    columns=vocab
)
preview_df.iloc[:, :20]

```


# Cargar el dataset

```{python}
#categorias = ['comp.graphics','comp.sys.mac.hardware','rec.sport.baseball','talk.politics.misc']

#newsgroups = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))
#print(newsgroups.target_names)
```

# Features y target

```{python}
#X_text = newsgroups.data #features
#y = newsgroups.target #target
```

```